{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f612e-b4da-497f-b54d-27c485b2f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from importlib import reload\n",
    "fpath = os.path.join('..//scripts')\n",
    "sys.path.append(fpath)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#loading internal scripts\n",
    "import frauddetection as fd\n",
    "import sourcedata as sd\n",
    "import countrymanagement as cm\n",
    "import mccmanagement as mccm\n",
    "reload(fd)\n",
    "reload(sd)\n",
    "reload(cm)\n",
    "reload(mccm)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace7ce9-638b-43f5-bb04-cc5d425ef5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#For Kaggle\n",
    "#date=''\n",
    "\n",
    "#For WL data\n",
    "date='20241118'\n",
    "source='WL'\n",
    "\n",
    "saveImg=False\n",
    "\n",
    "print('done')\n",
    "\n",
    "import dataimport as di\n",
    "import pandas as pd \n",
    "   \n",
    "dfTrx = di.read_file('../data/raw/'+source+'export'+date+'.csv')\n",
    "dfTrx.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322949db-e53e-471c-87db-7872a5be9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dfTrx['term_mcc'] = dfTrx['term_mcc'].astype(str)\n",
    "dfTrx['ecom'] = np.where(dfTrx.card_entry_mode.isin([5,6,9]), 'ECOM','FTF')\n",
    "dfTrx['wordV0']=dfTrx['term_country']\n",
    "dfTrx['wordV1']=dfTrx['term_mcc']\n",
    "dfTrx['wordV2']=dfTrx['term_country']+dfTrx['term_mcc']\n",
    "dfTrx['wordV3']=dfTrx['ecom']+dfTrx['term_country']+dfTrx['term_mcc']\n",
    "print(dfTrx.Class.value_counts(normalize=True))\n",
    "dfTrx[['wordV0','wordV1','wordV2','wordV3']].head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a4a4e-9fef-4cbc-8fdf-e1e860cfe317",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = dfTrx.sort_values(by=['card_pan_id','trx_date_time'])\n",
    "\n",
    "sorted_df['card_pan_id1'] = sorted_df['card_pan_id'].shift(-1)\n",
    "sorted_df['wordV0P'] = sorted_df['wordV0'].shift(-1)\n",
    "sorted_df['wordV1P'] = sorted_df['wordV1'].shift(-1)\n",
    "sorted_df['wordV2P'] = sorted_df['wordV2'].shift(-1)\n",
    "sorted_df['wordV3P'] = sorted_df['wordV3'].shift(-1)\n",
    "\n",
    "sorted_df['db_uuid1'] = sorted_df['db_uuid'].shift(-1)\n",
    "sorted_df[['wordV0','wordV0P','wordV1','wordV1P','wordV2','wordV2P','wordV3','wordV3P']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54debcfa-5182-4602-91ed-187b760a9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df1=sorted_df[(sorted_df['card_pan_id']==sorted_df['card_pan_id1'])]\n",
    "sorted_df0=sorted_df[(sorted_df['card_pan_id']!=sorted_df['card_pan_id1'])]\n",
    "\n",
    "print(sorted_df.shape)\n",
    "print(sorted_df0.shape)\n",
    "print(sorted_df1.shape)\n",
    "\n",
    "print(sorted_df.Class.value_counts(normalize=True))\n",
    "print(sorted_df0.Class.value_counts(normalize=True))\n",
    "print(sorted_df1.Class.value_counts(normalize=True))\n",
    "\n",
    "\n",
    "#sorted_df4.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2c977-06e1-4054-8c60-7e36f4a69a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dfTrx['previous_trx']=0\n",
    "\n",
    "sorted_dfTemp=sorted_df[(sorted_df['card_pan_id']==sorted_df['card_pan_id1'])]\n",
    "dfTrx['previous_trx']=np.where(sorted_df.index.isin(sorted_dfTemp.index),1,dfTrx['previous_trx'])\n",
    "\n",
    "dfTrx['card_pan_id1']=sorted_df['card_pan_id1']\n",
    "dfTrx['wordV0P'] = sorted_df['wordV0P']\n",
    "dfTrx['wordV1P'] = sorted_df['wordV1P']\n",
    "dfTrx['wordV2P'] = sorted_df['wordV2P']\n",
    "dfTrx['wordV3P'] = sorted_df['wordV3P']\n",
    "dfTrx['db_uuid1']=sorted_df['db_uuid1']\n",
    "\n",
    "print(dfTrx['previous_trx'].value_counts())\n",
    "dfTrx.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990f66c-f4fb-467c-8e11-ac4e1dce90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV0']!=dfTrx['wordV0P'])]\n",
    "temp2=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV0']==dfTrx['wordV0P'])]\n",
    "\n",
    "print(temp1.shape)\n",
    "print(temp2.shape)\n",
    "temp2[['wordV0','wordV0P']].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0095282-0428-4292-9a85-c4fc1601fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV1']!=dfTrx['wordV1P'])]\n",
    "temp2=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV1']==dfTrx['wordV1P'])]\n",
    "\n",
    "print(temp1.shape)\n",
    "print(temp2.shape)\n",
    "temp2[['wordV1','wordV1P']].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bbab03-918c-4d8f-964f-742bc916c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV2']!=dfTrx['wordV2P'])]\n",
    "temp2=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV2']==dfTrx['wordV2P'])]\n",
    "\n",
    "print(temp1.shape)\n",
    "print(temp2.shape)\n",
    "temp2[['wordV2','wordV2P']].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162d26c-0e90-406e-b1bd-b31eb8dea20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV3']!=dfTrx['wordV3P'])]\n",
    "temp2=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV3']==dfTrx['wordV3P'])]\n",
    "\n",
    "print(temp1.shape)\n",
    "print(temp2.shape)\n",
    "temp2[['wordV3','wordV3P']].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c262b-da47-4543-9874-35e6d947bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2=dfTrx[(dfTrx['card_pan_id']==dfTrx['card_pan_id1'])&(dfTrx['wordV3']!=dfTrx['wordV3P'])]\n",
    "temp2[['trx_date_time','card_pan_id','card_pan_id1','wordV1','wordV1P','wordV2','wordV2P','wordV3','wordV3P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28689a-82be-4f48-adb2-0ea0dc59c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "f0=open(\"../data/processed/wordV0.txt\",\"w+\") # file name and mode\n",
    "f1=open(\"../data/processed/wordV1.txt\",\"w+\") # file name and mode\n",
    "f2=open(\"../data/processed/wordV2.txt\",\"w+\") # file name and mode\n",
    "f3=open(\"../data/processed/wordV3.txt\",\"w+\") # file name and mode\n",
    "text0=''\n",
    "text1=''\n",
    "text2=''\n",
    "text3=''\n",
    "count=0\n",
    "count0=0\n",
    "count1=0\n",
    "count2=0\n",
    "count3=0\n",
    "for index,row in temp2.iterrows():\n",
    "    if((index % 5000)==0):\n",
    "        print(\"\",index)\n",
    "        print('count123',count,count0,count1,count2,count3)\n",
    "    if(row['card_pan_id']==row['card_pan_id1']):\n",
    "        count=count+1\n",
    "        if(row['wordV0P']!=row['wordV0']):\n",
    "            count0=count0+1\n",
    "            f0.write(row['wordV0P']+\" \"+row['wordV0']+'.\\n')\n",
    "            text0=text1+row['wordV0P']+\" \"+row['wordV0']+'.\\n'\n",
    "        if(row['wordV1P']!=row['wordV1']):\n",
    "            count1=count1+1\n",
    "            f1.write(row['wordV1P']+\" \"+row['wordV1']+'.\\n')\n",
    "            text1=text1+row['wordV1P']+\" \"+row['wordV1']+'.\\n'\n",
    "        if(row['wordV2P']!=row['wordV2']):\n",
    "            count2=count2+1\n",
    "            f2.write(row['wordV2P']+\" \"+row['wordV2']+'.\\n')\n",
    "            text2=text2+row['wordV2P']+\" \"+row['wordV2']+'.\\n'\n",
    "        if(row['wordV3P']!=row['wordV3']):\n",
    "            count3=count3+1\n",
    "            f3.write(row['wordV3P']+\" \"+row['wordV3']+'.\\n')\n",
    "            text3=text3+row['wordV3P']+\" \"+row['wordV3']+'.\\n'\n",
    "\n",
    "f0.close()\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "print('count123',count,count0,count1,count2,count3)\n",
    "print('done ',index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be7443-10f6-44e7-b8c1-29365246c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = open(\"../data/processed/wordV0.txt\", \"r\")\n",
    "text0=f0.read()\n",
    "f1 = open(\"../data/processed/wordV1.txt\", \"r\")\n",
    "text1=f1.read()\n",
    "f2 = open(\"../data/processed/wordV2.txt\", \"r\")\n",
    "text2=f2.read()\n",
    "f3 = open(\"../data/processed/wordV3.txt\", \"r\")\n",
    "text3=f3.read()\n",
    "f0.close()\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1a6f0-61d8-4864-8aed-5450c7efdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "token=word_tokenize(text0)\n",
    "tokenClean= [i for i in token if i is not '.']\n",
    "freq= nltk.FreqDist(tokenClean)\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa07fa-c676-4eeb-b117-62d3b40c0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "token=word_tokenize(text1)\n",
    "tokenClean= [i for i in token if i is not '.']\n",
    "freq= nltk.FreqDist(tokenClean)\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e9958-25ae-4ae1-b394-b1d7c3d95d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "token=word_tokenize(text2)\n",
    "tokenClean= [i for i in token if i is not '.']\n",
    "freq= nltk.FreqDist(tokenClean)\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd85b0-1797-45b9-82ff-cb7e14afcb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "token=word_tokenize(text3)\n",
    "tokenClean= [i for i in token if i is not '.']\n",
    "freq= nltk.FreqDist(tokenClean)\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48db51b-bfb4-42f0-a95a-7eca423ce9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_not_present(wordT,model,defaultValue):\n",
    "    if (wordT in model.wv.key_to_index):\n",
    "        return wordT\n",
    "    else:\n",
    "        print(wordT)\n",
    "        return defaultValue\n",
    "\n",
    "def calculDistance(input0,input1,input2,input3,model):\n",
    "    if(input0!=input1):\n",
    "        return 1.1\n",
    "    else:\n",
    "        #print(input0,input1,input2,input3\n",
    "        return model.wv.similarity(input2,input3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900801c9-bcd5-4e60-9fa3-ca08e3aaf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "data = []\n",
    " \n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(text0):\n",
    "    temp = []\n",
    " \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    " \n",
    "    data.append(temp)\n",
    "\n",
    "model0 = Word2Vec(data,min_count=1,window=2,max_vocab_size=2000000)\n",
    "import pickle\n",
    "pickle.dump(model0, open('../data/processed/wordV0Model', 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2786ea6-4d35-4e52-bab2-a15be920d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(dfTrx[['wordV0','wordV0P']].head(128))\n",
    "\n",
    "print(dfTrx[['wordV0','wordV0P','card_pan_id','card_pan_id1']].head(128))\n",
    "\n",
    "dfTrx['wordV0'] = dfTrx['wordV0'].apply(lambda x:word_not_present(x,model0,'BEL'))\n",
    "dfTrx['wordV0P'] = dfTrx['wordV0P'].apply(lambda x:word_not_present(x,model0,'BEL'))\n",
    "\n",
    "dfTrx['distancePrevTrxV0']= dfTrx.apply(lambda x: calculDistance(x.card_pan_id,x.card_pan_id1,x.wordV0P,x.wordV0,model0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d90fef-9945-4880-8baa-40add6213839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "data = []\n",
    " \n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(text1):\n",
    "    temp = []\n",
    " \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    " \n",
    "    data.append(temp)\n",
    "\n",
    "model1 = Word2Vec(data,min_count=1,window=2,max_vocab_size=2000000)\n",
    "import pickle\n",
    "pickle.dump(model1, open('../data/processed/wordV1Model', 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2b99d-3bba-4786-a55b-c2a0a950ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTrxSaved['distancePrevTrx']= \n",
    "#model.wv.similarity(dfTrxSaved['word'], dfTrxSaved['word1'])\n",
    "print(dfTrx[['wordV1','wordV1P']].head(128))\n",
    "\n",
    "print(dfTrx[['wordV1','wordV1P','card_pan_id','card_pan_id1']].head(128))\n",
    "\n",
    "dfTrx['wordV1'] = dfTrx['wordV1'].apply(lambda x:word_not_present(x,model1,'5411'))\n",
    "dfTrx['wordV1P'] = dfTrx['wordV1P'].apply(lambda x:word_not_present(x,model1,'5411'))\n",
    "\n",
    "dfTrx['distancePrevTrxV1']= dfTrx.apply(lambda x: calculDistance(x.card_pan_id,x.card_pan_id1,x.wordV1P,x.wordV1,model1), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2620756-1763-4011-b17c-cb5515744c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "data = []\n",
    " \n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(text2):\n",
    "    temp = []\n",
    " \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    " \n",
    "    data.append(temp)\n",
    "\n",
    "model2 = Word2Vec(data,min_count=1,window=2,max_vocab_size=2000000)\n",
    "import pickle\n",
    "pickle.dump(model2, open('../data/processed/wordV2Model', 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45eea4a-b2a1-4e5e-9cfc-05455f7733b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(dfTrx[['wordV2','wordV2P','card_pan_id','card_pan_id1']].head(128))\n",
    "\n",
    "dfTrx['wordV2'] = dfTrx['wordV2'].apply(lambda x:word_not_present(x,model2,'BEL5411'))\n",
    "dfTrx['wordV2P'] = dfTrx['wordV2P'].apply(lambda x:word_not_present(x,model2,'BEL5411'))\n",
    "\n",
    "dfTrx['distancePrevTrxV2']= dfTrx.apply(lambda x: calculDistance(x.card_pan_id,x.card_pan_id1,x.wordV2P,x.wordV2,model2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248886fd-be9a-405e-84e5-08e3c71c2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "data = []\n",
    " \n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(text3):\n",
    "    temp = []\n",
    " \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    " \n",
    "    data.append(temp)\n",
    "\n",
    "model3 = Word2Vec(data,min_count=1,window=2,max_vocab_size=2000000)\n",
    "pickle.dump(model3, open('../data/processed/wordV3Model', 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91a47e-f30e-4ad2-807d-96afe71eb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrx[['wordV3','wordV3P','card_pan_id','card_pan_id1']].head(128))\n",
    "    \n",
    "dfTrx['wordV3'] = dfTrx['wordV3'].apply(lambda x:word_not_present(x,model3,'FTFBEL5411'))\n",
    "dfTrx['wordV3P'] = dfTrx['wordV3P'].apply(lambda x:word_not_present(x,model3,'FTFBEL5411'))\n",
    "\n",
    "dfTrx['distancePrevTrxV3']= dfTrx.apply(lambda x: calculDistance(x.card_pan_id,x.card_pan_id1,x.wordV3P,x.wordV3,model3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40300089-9b95-47ef-8e06-e909df9ac0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrx[['distancePrevTrxV0','distancePrevTrxV1','distancePrevTrxV2','distancePrevTrxV3']].head(12))\n",
    "print(dfTrx['distancePrevTrxV0'].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']==1.0)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<1.0)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.9)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.8)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.7)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.6)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.5)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.4)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.3)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.2)].shape)\n",
    "print(dfTrx['distancePrevTrxV0'][(dfTrx['distancePrevTrxV0']<0.1)].shape)\n",
    "\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']==1.1)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<=1.0)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<1.0)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<0.9)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<0.8)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<0.7)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<0.6)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<0.5)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<0.4)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<0.3)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV0']<0.2)].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f921155-6ff5-4cc1-adf3-990293a9ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrx[['distancePrevTrxV1','distancePrevTrxV2','distancePrevTrxV3']].head(8))\n",
    "print(dfTrx['distancePrevTrxV1'].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']==1.0)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<1.0)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.9)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.8)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.7)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.6)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.5)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.4)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.3)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.2)].shape)\n",
    "print(dfTrx['distancePrevTrxV1'][(dfTrx['distancePrevTrxV1']<0.1)].shape)\n",
    "\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']==1.1)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<=1.0)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<1.0)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<0.9)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<0.8)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<0.7)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<0.6)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<0.5)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<0.4)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<0.3)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV1']<0.2)].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f239be-a4ce-4341-872b-859653a8d003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max = 8\n",
    "\n",
    "print('distancePrevTrxV0 term_country')\n",
    "for bins in np.arange(2,max,1):\n",
    "    dfTrx['distancePrevTrxBinV0']=pd.cut(dfTrx['distancePrevTrxV0'], bins=bins)\n",
    "    print(f'IV dist bins={bins} {fd.calc_iv(dfTrx,'distancePrevTrxBinV0','Class',pr=0)}')\n",
    "\n",
    "print('distancePrevTrxV1 term_mcc')\n",
    "for bins in np.arange(2,max,1):\n",
    "    dfTrx['distancePrevTrxBinV1']=pd.cut(dfTrx['distancePrevTrxV1'], bins=bins)\n",
    "    print(f'IV dist bins={bins} {fd.calc_iv(dfTrx,'distancePrevTrxBinV1','Class',pr=0)}')\n",
    "\n",
    "print('---------------------')\n",
    "print('distancePrevTrxV2 term_country term_mcc')\n",
    "for bins in np.arange(2,max,1):\n",
    "    dfTrx['distancePrevTrxBinV2']=pd.cut(dfTrx['distancePrevTrxV2'], bins=bins)\n",
    "    print(f'IV dist bins={bins} {fd.calc_iv(dfTrx,'distancePrevTrxBinV2','Class',pr=0)}')\n",
    "\n",
    "\n",
    "print('---------------------')\n",
    "print('distancePrevTrxV3 ecom term_country term_mcc')\n",
    "for bins in np.arange(2,max,1):\n",
    "    dfTrx['distancePrevTrxBinV3']=pd.cut(dfTrx['distancePrevTrxV3'], bins=bins)\n",
    "    print(f'IV dist bins={bins} {fd.calc_iv(dfTrx,'distancePrevTrxBinV3','Class',pr=0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c452d0-6894-4f0b-81a3-eb0dec1a3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrx['distancePrevTrxV2'].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']==1.0)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<1.0)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.9)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.8)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.7)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.6)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.5)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.4)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.3)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.2)].shape)\n",
    "print(dfTrx['distancePrevTrxV2'][(dfTrx['distancePrevTrxV2']<0.1)].shape)\n",
    "\n",
    "print('All ',dfTrx['Class'].value_counts(normalize=True))\n",
    "print('=1.1',dfTrx['Class'][(dfTrx['distancePrevTrxV2']==1.1)].value_counts(normalize=True))\n",
    "print('<=1.0',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<=1.0)].value_counts(normalize=True))\n",
    "print('<1.0',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<1.0)].value_counts(normalize=True))\n",
    "print('<0.9',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.9)].value_counts(normalize=True))\n",
    "print('<0.8',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.8)].value_counts(normalize=True))\n",
    "print('<0.7',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.7)].value_counts(normalize=True))\n",
    "print('<0.6',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.6)].value_counts(normalize=True))\n",
    "print('<0.5',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.5)].value_counts(normalize=True))\n",
    "print('<0.4',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.4)].value_counts(normalize=True))\n",
    "print('<0.3',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.3)].value_counts(normalize=True))\n",
    "print('<0.2',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.2)].value_counts(normalize=True))\n",
    "print('<0.1',dfTrx['Class'][(dfTrx['distancePrevTrxV2']<0.1)].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33630c6-3fde-4043-bbbc-f9f746dbcfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<1.0)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.9)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.8)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.7)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.6)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.5)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.4)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.3)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.2)].shape)\n",
    "print(dfTrx['distancePrevTrxV3'][(dfTrx['distancePrevTrxV3']<0.1)].shape)\n",
    "\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']==1.0)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<1.0)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<0.9)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<0.8)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<0.7)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<0.6)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<0.5)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<0.4)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<0.3)].value_counts(normalize=True))\n",
    "print(dfTrx['Class'][(dfTrx['distancePrevTrxV3']<0.2)].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7017c-4e42-4132-8cbe-4a67e8691123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max = 11\n",
    "\n",
    "print('distancePrevTrxV2')\n",
    "for bins in np.arange(2,max,1):\n",
    "    dfTrx['distancePrevTrxBin']=pd.cut(dfTrx['distancePrevTrxV3'], bins=bins)\n",
    "    print(f'IV trx_local_amt_val bins={bins} {fd.calc_iv(dfTrx,'distancePrevTrxBinV3','Class',pr=0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e3c34-07d7-4473-847e-58f090891e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.wv.similar_by_word('5411'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2686c-4524-4ec5-985c-b45f8d0d5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.wv.similar_by_word('5812'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc27748-2055-4f31-a743-8b663c327c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.wv.similar_by_word('5818'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710d926-a720-43b9-86a6-8058e3616194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.wv.similar_by_word('5999'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba2a5b-e688-4ccb-85ba-a04f7f77ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model0.wv.similar_by_word('BEL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d51bb-e6ca-492d-9dcd-fa8c9778463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model0.wv.similar_by_word('FRA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee146a-88ea-44a0-ac57-b62c42fa9bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model0.wv.similar_by_word('IRL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917be7a-5883-4b14-aea5-9bb6ec341eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
